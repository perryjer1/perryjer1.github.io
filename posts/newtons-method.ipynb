{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newton's Method\n",
    "\n",
    "After posting the [R code for calculating risk parity weights](link://slug/risk-parity-weights-in-r), I thought I should review Newton's method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate\n",
    "\n",
    "First, from the definition of a derivative for a function, we have\n",
    "\n",
    "$\\frac{f(x+h) - f(x)}{h} = f'(x) + E(x,h)$\n",
    "\n",
    "Here $E$ is the error introduced by the approximation\n",
    "\n",
    "$f(x+h) \\approx f(x) + h f'(x)$\n",
    "\n",
    "and if the derivative $f'(x)$ exists, $E$ must obey $ \\lim_{h \\to 0} E(x,h) = 0$.\n",
    "\n",
    "Rearranging the equation gives\n",
    "\n",
    "$f(x+h) = f(x) + hf'(x) + hE(x,h)$\n",
    "\n",
    "This is a form of the first order Taylor polynomial. Since $E \\rightarrow 0$ as $h \\rightarrow 0$, $hE(x,h)$ is \"small\".\n",
    "\n",
    "The goal is to find a zero, i.e. some $a$ such that $f(a) = 0$. Using the above and the smallness of $hE(x,h)$, a first order approximation for the zero will be to find $h$ such that $f(x+h) = 0$. Using that above we get\n",
    "\n",
    "$0 = f(x) + hf'(x)$\n",
    "\n",
    "Solving for $h$ gives\n",
    "\n",
    "$h = -\\frac{f(x)}{f'(x)}$\n",
    "\n",
    "so our guess for the root $a = x+h$ is\n",
    "\n",
    "$x - \\frac{f(x)}{f'(x)}$\n",
    "\n",
    "Making successive approximations yields a sequence\n",
    "\n",
    "$x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate\n",
    "\n",
    "The first order Taylor polynomial has a multivariate generalization:\n",
    "\n",
    "$f(x+h) = f(x) + T_x(h) + \\|h\\| E(x,h)$\n",
    "\n",
    "where $T_x(h)$ is the total derivative which is represented by the Jacobian matrix, a matrix where row $i$ and column $j$ is\n",
    "\n",
    "$\\frac{\\partial f_i}{\\partial x_j}$,\n",
    "\n",
    "the partial derivative of the function $i$ with respect to variable $j$. We are dealing in higher dimensions now so $x$ and $h$ are vectors and $f$ is a function $\\mathbb{R}^n \\rightarrow \\mathbb{R}^m$. We will be inverting the Jacobian so we will in fact need $n = m$.\n",
    "\n",
    "Our first order approximation becomes\n",
    "\n",
    "$0 = f(x) + J(x) \\cdot h$\n",
    "\n",
    "Solving for $h$ gives\n",
    "\n",
    "$h = -[J(x)]^{-1} f(x)$\n",
    "\n",
    "and so the guess for the roots become\n",
    "\n",
    "$x - [J(x)]^{-1} f(x)$\n",
    "\n",
    "As a sequence, we can write\n",
    "\n",
    "$x_{n+1} = x_n - [J(x_n)]^{-1} f(x_n)$\n",
    "\n",
    "This is the form of the solver used in the paper."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nikola": {
   "category": "",
   "date": "2017-05-12 09:33:57 UTC-05:00",
   "description": "",
   "link": "",
   "slug": "newtons-method",
   "tags": "math",
   "title": "Newton's Method",
   "type": "text"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
