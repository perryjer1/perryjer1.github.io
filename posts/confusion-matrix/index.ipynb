{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are my notes on the [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix). That page has a lot of detail and I found it useful to work through a specific example.\n",
    "\n",
    "<!-- TEASER_END -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple `knn` model\n",
    "\n",
    "To create a confusion matrix, we will use a `k`-nearest neighbors model to predict default on the `Caravan` data set from `ISLR`. See section 4.6 in the [ISLR book](http://www-bcf.usc.edu/~gareth/ISL/) for details. We will be focused on just using the actual vs. predicted result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(ISLR)\n",
    "library(class)\n",
    "\n",
    "## Load data and split into test and training\n",
    "X <- scale(Caravan[, -86])\n",
    "test <- 1:1000\n",
    "trX <- X[-test,]  # training data\n",
    "teX <- X[test,]   # test data\n",
    "trY <- Caravan$Purchase[-test]  # training target\n",
    "teY <- Caravan$Purchase[test]   # test target\n",
    "\n",
    "## knn uses some randomness e.g. to break ties, set seed for reproducibility\n",
    "set.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the target variable, 'Yes' means a default occured. For reference, the probability of being 'Yes' in the test data is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.9 %\n"
     ]
    }
   ],
   "source": [
    "cat(mean(teY == \"Yes\")*100, \"%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a model using all predictors and 3 nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      predicted\n",
       "actual   No  Yes  Sum\n",
       "   No   921   20  941\n",
       "   Yes   54    5   59\n",
       "   Sum  975   25 1000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model <- knn(trX, teX, trY, k = 3)\n",
    "\n",
    "confusion_matrix <- function(actual, predicted) {\n",
    "  x <- addmargins(table(actual = teY, predicted = model))\n",
    "  # jupyter notebooks drop some dimnames when printing because\n",
    "  # 'matrix' gets added to the classes after a call to addmargins,\n",
    "  # just reset the class to 'table' and it is fine\n",
    "  class(x) <- \"table\"\n",
    "  x\n",
    "}\n",
    "\n",
    "confusion_matrix(actual = teY, predicted = model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix\n",
    "\n",
    "The last result is a confusion matrix. It shows actual vs. predicted. Here is a generalized version of a confusion matrix with variable names we will use below. `Neg` and `Pos` are somewhat arbitrary names but they come from terminology like \"false positive\".\n",
    "\n",
    "| Confusion matrix |       |             |     |       |\n",
    "|------------------|-------|-------------|-----|-------|\n",
    "|                  |       | Predicted   |     |       |\n",
    "|                  |       | Neg         | Pos | Total |\n",
    "| Actual           | Neg   | TN          | FP  | N     |\n",
    "|                  | Pos   | FN          | TP  | P     |\n",
    "|                  | Total | N\\*         | P\\* | T     |\n",
    "\n",
    "**True negatives** (TN) are predicted negative and are actually negative. **True positives** (TP) are predicted positive and are actually positive.\n",
    "\n",
    "**False negatives** (FN) are predicted negative but are actually positive. **False positives** (FP) are predicted positive but actually negative.\n",
    "\n",
    "The **accuracy** is the fraction of samples classified correctly whereas the **error** is the fraction of samples misclassified. Hence, $accuracy = 1 - error$.\n",
    "\n",
    "The **specificity**, aka true negative rate, is the fraction of actual negative cases that were predicted to be negative.\n",
    "\n",
    "The **sensitivity**, aka true positive rate, recall, or power, is the fraction of actual positive cases that were predicted to be positive.\n",
    "\n",
    "A **Type I error** occurs when we predict positive but it is actually negative. The false positive rate is the fraction of actually negative cases that were predicted to be positive. This is also $1 - specificity$.\n",
    "\n",
    "A **Type II error** occurs when we predict negative but it is actually positive. The false negative rate is the fraction of actually positive cases that were predicted to be negative. This is also $1 - sensitivity$.\n",
    "\n",
    "The **precision** is the fraction of predicted true cases that are actually true.\n",
    "\n",
    "The **false discovery rate (FDR)** is the fraction of predicted true cases that are actually false.\n",
    "\n",
    "Here is a table that summarizes these names and a few aliases etc.\n",
    "\n",
    "| Formula       | Alt      | Names                     |               |               |                 |       |\n",
    "|---------------|----------|---------------------------|---------------|---------------|-----------------|-------|\n",
    "| (TN+TP)/T     |          | Accuracy                  |               |               |                 |       |\n",
    "| (FN+FP)/T     |          | Error                     |               |               |                 |       |\n",
    "| TN/(TN+FP)    | TN / N   | True negative rate        | Specificity   |               | 1-Type I error  |       |\n",
    "| FP/(TN+FP)    | FP / N   | False positive rate       | 1-Specificity |               | Type I error    |       |\n",
    "| FN/(FN+TP)    | FN / P   | False negative rate       | 1-Sensitivity |               | Type II error   |       |\n",
    "| TP/(FN+TP)    | TP / P   | True positive rate        | Sensitivity   | Recall        | 1-Type II error | Power |\n",
    "| TN/(TN+FN)    | TN / N\\* | Negative predictive value |               |               |                 |       |\n",
    "| FN/(TN+FN)    | FN / N\\* | False omission rate       |               |               |                 |       |\n",
    "| FP/(FP+TP)    | FP / P\\* | False discovery rate      |               | 1 - Precision |                 |       |\n",
    "| TP/(FP+TP)    | TP / P\\* | Positive predictive value |               | Precision     |                 |       |\n",
    "\n",
    "Finally, here is an R function that summarizes our discussion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      predicted\n",
       "actual   No  Yes  Sum\n",
       "   No   921   20  941\n",
       "   Yes   54    5   59\n",
       "   Sum  975   25 1000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>Name</th><th scope=col>Formula</th><th scope=col>Result</th><th scope=col>Numeric</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>Accuracy   </td><td>(TN+TP)/T  </td><td>926/1000   </td><td>92.60%     </td></tr>\n",
       "\t<tr><td>Error      </td><td>(FN+FP)/T  </td><td>74/1000    </td><td>7.40%      </td></tr>\n",
       "\t<tr><td>Specificity</td><td>TN / N     </td><td>921/941    </td><td>97.87%     </td></tr>\n",
       "\t<tr><td>Sensitivity</td><td>TP / P     </td><td>5/59       </td><td>8.47%      </td></tr>\n",
       "\t<tr><td>False Disc.</td><td>FP / P*    </td><td>20/40      </td><td>50.00%     </td></tr>\n",
       "\t<tr><td>Precision  </td><td>TP / P*    </td><td>5/40       </td><td>12.50%     </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       " Name & Formula & Result & Numeric\\\\\n",
       "\\hline\n",
       "\t Accuracy    & (TN+TP)/T   & 926/1000    & 92.60\\%   \\\\\n",
       "\t Error       & (FN+FP)/T   & 74/1000     & 7.40\\%    \\\\\n",
       "\t Specificity & TN / N      & 921/941     & 97.87\\%   \\\\\n",
       "\t Sensitivity & TP / P      & 5/59        & 8.47\\%    \\\\\n",
       "\t False Disc. & FP / P*     & 20/40       & 50.00\\%   \\\\\n",
       "\t Precision   & TP / P*     & 5/40        & 12.50\\%   \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "Name | Formula | Result | Numeric | \n",
       "|---|---|---|---|---|---|\n",
       "| Accuracy    | (TN+TP)/T   | 926/1000    | 92.60%      | \n",
       "| Error       | (FN+FP)/T   | 74/1000     | 7.40%       | \n",
       "| Specificity | TN / N      | 921/941     | 97.87%      | \n",
       "| Sensitivity | TP / P      | 5/59        | 8.47%       | \n",
       "| False Disc. | FP / P*     | 20/40       | 50.00%      | \n",
       "| Precision   | TP / P*     | 5/40        | 12.50%      | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  Name        Formula   Result   Numeric\n",
       "1 Accuracy    (TN+TP)/T 926/1000 92.60% \n",
       "2 Error       (FN+FP)/T 74/1000  7.40%  \n",
       "3 Specificity TN / N    921/941  97.87% \n",
       "4 Sensitivity TP / P    5/59     8.47%  \n",
       "5 False Disc. FP / P*   20/40    50.00% \n",
       "6 Precision   TP / P*   5/40     12.50% "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "confusion_stats <- function(actual, predicted) {\n",
    "  tab <- table(actual = actual, predicted = predicted)\n",
    "    \n",
    "  TN <- tab[1,1]  # true negatives\n",
    "  TP <- tab[2,2]  # true positives\n",
    "  FN <- tab[2,1]  # false negatives\n",
    "  FP <- tab[1,2]  # false positives\n",
    "  N <- TN + FP    # actual negatives\n",
    "  P <- FN + TP    # actual positives\n",
    "  PN <- TN + FN   # predicted negatives, N*\n",
    "  PP <- FP + FP   # predicted positives, P*\n",
    "  T <- sum(tab)\n",
    "  \n",
    "  x <- data.frame(\n",
    "      rbind(c(\"Accuracy\",    \"(TN+TP)/T\", sprintf(\"%s/%s\", TN+TP, T), sprintf(\"%.2f%%\", 100*(TN+TP)/T)),\n",
    "            c(\"Error\",       \"(FN+FP)/T\", sprintf(\"%s/%s\", FN+FP, T), sprintf(\"%.2f%%\", 100*(FN+FP)/T)),\n",
    "            c(\"Specificity\", \"TN / N\",    sprintf(\"%s/%s\", TN, N),    sprintf(\"%.2f%%\", 100*TN/N)),\n",
    "            c(\"Sensitivity\", \"TP / P\",    sprintf(\"%s/%s\", TP, P),    sprintf(\"%.2f%%\", 100*TP/P)),\n",
    "            c(\"False Disc.\", \"FP / P*\",   sprintf(\"%s/%s\", FP, PP),   sprintf(\"%.2f%%\", 100*FP/PP)),\n",
    "            c(\"Precision\",   \"TP / P*\",   sprintf(\"%s/%s\", TP, PP),   sprintf(\"%.2f%%\", 100*TP/PP))))\n",
    "  colnames(x) <- c(\"Name\", \"Formula\", \"Result\", \"Numeric\")\n",
    "  x\n",
    "}\n",
    "\n",
    "# repeat the confusion matrix here for reference\n",
    "confusion_matrix(actual = teY, predicted = model)\n",
    "confusion_stats(actual = teY, predicted = model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R 3.3",
   "language": "R",
   "name": "ir33"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.3"
  },
  "nikola": {
   "category": "",
   "date": "2017-06-02 14:56:43 UTC-05:00",
   "description": "",
   "link": "",
   "slug": "confusion-matrix",
   "tags": "",
   "title": "Confusion Matrix",
   "type": "text"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
